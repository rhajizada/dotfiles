{
  "$schema": "https://opencode.ai/config.json",
  "mcp": {
    "duckduckgo": {
      "type": "local",
      "command": ["docker", "run", "-i", "--rm", "mcp/duckduckgo"],
      "enabled": true,
    },
    "fetch": {
      "type": "local",
      "command": ["docker", "run", "-i", "--rm", "mcp/fetch"],
      "enabled": true,
    },
    "next-devtools-mcp": {
      "type": "local",
      "command": ["docker", "run", "-i", "--rm", "mcp/next-devtools-mcp"],
      "enabled": true,
    },
    "openapi": {
      "type": "local",
      "command": ["docker", "run", "-i", "--rm", "-e", "MODE", "mcp/openapi"],
      "enabled": true,
      "environment": {
        "MODE": "Stdio",
      },
    },
    "sequentialthinking": {
      "type": "local",
      "command": [
        "docker",
        "run",
        "-i",
        "--rm",
        "-e",
        "MODE",
        "mcp/sequentialthinking",
      ],
      "enabled": true,
      "environment": {
        "MODE": "Stdio",
      },
    },
  },
  "provider": {
    "llamero": {
      "npm": "@ai-sdk/openai-compatible",
      "name": "Llamero",
      "options": {
        "baseURL": "{env:LLAMERO_API_BASE}",
        "apiKey": "{env:LLAMERO_API_KEY}",
      },
      "models": {
        "llama3.2:3b": {
          "name": "Llama3.2 3b",
          "tool_call": true,
          "limit": {
            "context": 131072,
            "output": 4096,
          },
        },
        "qwen3:4b": {
          "name": "Qwen3 4b",
          "reasoning": true,
          "tool_call": true,
          "limit": {
            "context": 81920,
            "output": 4096,
          },
        },
        "qwen3:8b": {
          "name": "Qwen3 8b",
          "reasoning": true,
          "tool_call": true,
          "limit": {
            "context": 40960,
            "output": 4096,
          },
        },
        "ministral-3:8b": {
          "name": "Ministral 3 8b",
          "tool_call": true,
          "limit": {
            "context": 57344,
            "output": 4096,
          },
        },
      },
    },
    "LM Studio": {
      "npm": "@ai-sdk/openai-compatible",
      "name": "LM Studio",
      "options": {
        "baseURL": "http://127.0.0.1:1234/v1",
      },
      "models": {
        "zai-org/glm-4.6v-flash": {
          "name": "GLM 4.6v Flash 9b",
          "reasoning": true,
          "tool_call": true,
          "limit": {
            "context": 32768,
            "output": 4096,
          },
        },
      },
    },
  },
  "plugin": ["opencode-handoff"],
}
